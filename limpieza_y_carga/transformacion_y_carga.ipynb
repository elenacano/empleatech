{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **1. Llamada a la API de OpenAI**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para cada búsqueda realizada de los distintos empleos en Madrid hemos obtenido varias ofertas de trabajo, de las cuales tenemos su correspondiente descripción. Para cada descripción se pretende obtener cúanto se ajusta dicha oferta al puesto para el cual se realizó la búsqueda, por ejemplo cuanto se ajusta la descripción a un puesto de Data Science, cuál es el nivel de inglés si se pidiera, los años de experiencia si se pidieran, las hard skill necesarias y las hard skills deseables.\n",
    "\n",
    "Para llevar a cabo esta tarea usaremos IA generativa a través de la API de OpenAI, la cual se encargará de llevar a cabo la labor de extraer de cada oferta la información deseada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "#from googletrans import Translator\n",
    "from difflib import SequenceMatcher\n",
    "from dotenv import load_dotenv\n",
    "import ast\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import json\n",
    "import os\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "from src import funciones_transformacion as f_transform\n",
    "from src import funciones_bbdd as f_bbdd\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La función de `extracion_datos_descripciones` para cada uno de los archivos finales obtenidos en la parte de extracción se encargá de extraer de la descripción los años de experiencia que piden, el nivel de inglés, las hard skills necesarias, las hard skill deseables y un porcentaje de ajuste. Este porcetaje representa cuanto se ajusta la descripción de la oferta en cuanto al puesto para el que se realizó la busqueda, por ejemplo, Data analyst. Todos estos datos se añadirán a un json que se irá almacenando en la carpeta `data/response_openai` en este mismo directorio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/ofertas_sin_duplicados/infojobs_data_analyst.csv\n",
      "data/ofertas_sin_duplicados/infojobs_data_engineer.csv\n",
      "data/ofertas_sin_duplicados/infojobs_data_science.csv\n",
      "data/ofertas_sin_duplicados/linkedin_data_analyst.csv\n",
      "data/ofertas_sin_duplicados/linkedin_data_engineer.csv\n",
      "data/ofertas_sin_duplicados/linkedin_data_science.csv\n"
     ]
    }
   ],
   "source": [
    "lista_ofertas=[]\n",
    "folder_path_linkedin = \"data/ofertas_sin_duplicados/\"\n",
    "files_linkedin = os.listdir(folder_path_linkedin)\n",
    "[lista_ofertas.append(folder_path_linkedin + file) for file in files_linkedin]\n",
    "\n",
    "for file_path in lista_ofertas:\n",
    "    print(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "Analizando el archivo -infojobs_data_analyst.csv- de la categoria: Data analyst, con shape: (111, 9)\n",
      "Analizando el archivo -infojobs_data_engineer.csv- de la categoria: Data engineer, con shape: (83, 9)\n",
      "Analizando el archivo -infojobs_data_science.csv- de la categoria: Data science, con shape: (61, 9)\n",
      "Analizando el archivo -linkedin_data_analyst.csv- de la categoria: Data analyst, con shape: (905, 8)\n",
      "Analizando el archivo -linkedin_data_engineer.csv- de la categoria: Data engineer, con shape: (854, 8)\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 22] Invalid argument: 'data/response_openai/transformed_linkedin_data_engineer.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mf_transform\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextracion_datos_descripciones\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Elena\\OneDrive\\Documentos\\empleatech_v2\\limpieza_y_carga\\..\\src\\funciones_transformacion.py:342\u001b[0m, in \u001b[0;36mextracion_datos_descripciones\u001b[1;34m()\u001b[0m\n\u001b[0;32m    340\u001b[0m \u001b[38;5;66;03m# Agregar la fila al listado de datos en JSON y guardarla\u001b[39;00m\n\u001b[0;32m    341\u001b[0m json_data\u001b[38;5;241m.\u001b[39mappend(row_data)\n\u001b[1;32m--> 342\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/response_openai/transformed_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnombre_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.json\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m json_file:\n\u001b[0;32m    343\u001b[0m     json\u001b[38;5;241m.\u001b[39mdump(json_data, json_file, ensure_ascii\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)\n",
      "\u001b[1;31mOSError\u001b[0m: [Errno 22] Invalid argument: 'data/response_openai/transformed_linkedin_data_engineer.json'"
     ]
    }
   ],
   "source": [
    "f_transform.extracion_datos_descripciones()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'charmap' codec can't decode byte 0x9d in position 20404: character maps to <undefined>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m nombre_file\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlinkedin_data_analyst\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/response_openai/transformed_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnombre_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.json\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m archivo:\n\u001b[1;32m----> 3\u001b[0m             json_datos \u001b[38;5;241m=\u001b[39m \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43marchivo\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\json\\__init__.py:293\u001b[0m, in \u001b[0;36mload\u001b[1;34m(fp, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    274\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(fp, \u001b[38;5;241m*\u001b[39m, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, object_hook\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, parse_float\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    275\u001b[0m         parse_int\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, parse_constant\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, object_pairs_hook\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw):\n\u001b[0;32m    276\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Deserialize ``fp`` (a ``.read()``-supporting file-like object containing\u001b[39;00m\n\u001b[0;32m    277\u001b[0m \u001b[38;5;124;03m    a JSON document) to a Python object.\u001b[39;00m\n\u001b[0;32m    278\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    291\u001b[0m \u001b[38;5;124;03m    kwarg; otherwise ``JSONDecoder`` is used.\u001b[39;00m\n\u001b[0;32m    292\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loads(\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m    294\u001b[0m         \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcls\u001b[39m, object_hook\u001b[38;5;241m=\u001b[39mobject_hook,\n\u001b[0;32m    295\u001b[0m         parse_float\u001b[38;5;241m=\u001b[39mparse_float, parse_int\u001b[38;5;241m=\u001b[39mparse_int,\n\u001b[0;32m    296\u001b[0m         parse_constant\u001b[38;5;241m=\u001b[39mparse_constant, object_pairs_hook\u001b[38;5;241m=\u001b[39mobject_pairs_hook, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\encodings\\cp1252.py:23\u001b[0m, in \u001b[0;36mIncrementalDecoder.decode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, final\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m---> 23\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m codecs\u001b[38;5;241m.\u001b[39mcharmap_decode(\u001b[38;5;28minput\u001b[39m,\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merrors,decoding_table)[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'charmap' codec can't decode byte 0x9d in position 20404: character maps to <undefined>"
     ]
    }
   ],
   "source": [
    "\n",
    "nombre_file=\"linkedin_data_analyst\"\n",
    "with open(f\"data/response_openai/transformed_{nombre_file}.json\", 'r',  encoding='utf-8') as archivo:\n",
    "            json_datos = json.load(archivo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Limpieza de los JSON**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez tenemos las skills de cada oferta hay que hacer un primer filtrado general de estas skills pues en total tenemos alrededor de 3000. Por lo tanto vamos a agrupar skills que se refieren a una herramienta pero han sido recogidas con distintos nombres en cada oferta como: PowerBi, Power BI, experto en PowerBi, todo ello lo pasaremos a un mismo término y asi se pretende reducir el número de skill."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtrando las skills del archivo: transformed_infojobs_data_analyst.json\n",
      "Filtrando las skills del archivo: transformed_infojobs_data_engineer.json\n",
      "Filtrando las skills del archivo: transformed_infojobs_data_science.json\n",
      "Filtrando las skills del archivo: transformed_linkedin_data_analyst.json\n",
      "Filtrando las skills del archivo: transformed_linkedin_data_engineer.json\n",
      "Filtrando las skills del archivo: transformed_linkedin_data_science.json\n",
      "\n",
      "Skills filtradas y guardadas correctamente.\n"
     ]
    }
   ],
   "source": [
    "f_transform.extraer_y_guardar_skills_filtradas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comprobamos cuantas skills en total había antes y después del filtrado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contando skills de los archivos: ['transformed_infojobs_data_analyst.json', 'transformed_infojobs_data_engineer.json', 'transformed_infojobs_data_science.json', 'transformed_linkedin_data_analyst.json', 'transformed_linkedin_data_engineer.json', 'transformed_linkedin_data_science.json']\n",
      "Se han encontrado 3210 skills diferentes.\n",
      "\n",
      "Contando skills de los archivos: ['filtered_transformed_infojobs_data_analyst.json', 'filtered_transformed_infojobs_data_engineer.json', 'filtered_transformed_infojobs_data_science.json', 'filtered_transformed_linkedin_data_analyst.json', 'filtered_transformed_linkedin_data_engineer.json', 'filtered_transformed_linkedin_data_science.json']\n",
      "Se han encontrado 2464 skills diferentes.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lista_skills = f_transform.contador_skills(\"data/response_openai/\")\n",
    "\n",
    "lista_skills = f_transform.contador_skills(\"data/skills_filtradas/\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hemos conseguido reducir el número total de skills unas 800, sin embargo, seguir manteniendo 2400 skills es inviable, pues para que el recomendador funcione el usuario deberá chequear si tiene conocimientos en cada una de ellas o no, y revisar una lista de más de 2000 skills no es una opción realista.\n",
    "\n",
    "Por lo tanto, para cada uno de los tres empleos vamos a ver cuales son las skills que más se repiten y haremos una lista con únicamente las más importante para cada oferta de trabajo. Tendremos que filtrar nuevamente las ofertas y dejar solo las skills que estén en la lista de hard skills generada en el paso anterior y la cual podremos encontrar en `data/lista_hard_skills.pkl`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se han guardado 110 skills relevantes en 'data/lista_hard_skills.pkl'\n",
      "\n",
      "Limpiando las skills de las ofertas, manteniendo solo las más relevantes...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 108/108 [00:00<00:00, 232.39it/s]\n",
      "100%|██████████| 73/73 [00:00<00:00, 292.03it/s]\n",
      "100%|██████████| 39/39 [00:00<00:00, 471.65it/s]\n",
      "100%|██████████| 200/200 [00:01<00:00, 120.84it/s]\n",
      "100%|██████████| 200/200 [00:01<00:00, 120.60it/s]\n",
      "100%|██████████| 200/200 [00:01<00:00, 120.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Completado!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "skills_relevantes = f_transform.filtrar_skills_min_apariciones(5)\n",
    "f_transform.filtracion_hard_skills_relevantes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comprobamos que efectivamente tras el filtrado nos ha quedado el mismo número de skills que en la lista:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contando skills de los archivos: ['relevant_filtered_transformed_infojobs_data_analyst.json', 'relevant_filtered_transformed_infojobs_data_engineer.json', 'relevant_filtered_transformed_infojobs_data_science.json', 'relevant_filtered_transformed_linkedin_data_analyst.json', 'relevant_filtered_transformed_linkedin_data_engineer.json', 'relevant_filtered_transformed_linkedin_data_science.json']\n",
      "Se han encontrado 110 skills diferentes.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lista_skills = f_transform.contador_skills(\"data/skills_filtradas_relevantes/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **2. Carga de datos en MongoDB**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez hemos limpiado las skills de todas las ofertas de trabajo y las hemos almacenado en los correspondientes `json` el siguiente paso es subir todos estos datos a una base de datos en la nube. En este caso al ser tablas no relacionales hemos escogido MongoDB y los datos se subiran a MongoDB Atlas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pinged your deployment. You successfully connected to MongoDB!\n",
      "Insertando file: relevant_relevant_filtered_transformed_infojobs_data_analyst.json\n",
      "Insertando file: relevant_relevant_filtered_transformed_infojobs_data_engineer.json\n",
      "Insertando file: relevant_relevant_filtered_transformed_infojobs_data_science.json\n",
      "Insertando file: relevant_relevant_filtered_transformed_linkedin_data_analyst.json\n",
      "Insertando file: relevant_relevant_filtered_transformed_linkedin_data_engineer.json\n",
      "Insertando file: relevant_relevant_filtered_transformed_linkedin_data_science.json\n",
      "---- Insercion en la base de datos finalizada ----\n",
      "\n",
      "Pinged your deployment. You successfully connected to MongoDB!\n",
      "Skills insertadas en la base de datos correctamente.\n"
     ]
    }
   ],
   "source": [
    "f_bbdd.cargar_datos_mongo()\n",
    "f_bbdd.cargar_skills_mongo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
