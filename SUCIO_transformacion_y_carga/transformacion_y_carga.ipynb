{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **1. Llamada a la API de OpenAI**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para cada búsqueda realizada de los distintos empleos en Madrid hemos obtenido varias ofertas de trabajo, de las cuales tenemos su correspondiente descripción. Para cada descripción se pretende obtener cúanto se ajusta dicha oferta al puesto para el cual se realizó la búsqueda, por ejemplo cuanto se ajusta la descripción a un puesto de Data Science, cuál es el nivel de inglés si se pidiera, los años de experiencia si se pidieran, las hard skill necesarias y las hard skills deseables.\n",
    "\n",
    "Para llevar a cabo esta tarea usaremos IA generativa a través de la API de OpenAI, la cual se encargará de llevar a cabo la labor de extraer de cada oferta la información deseada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import ast\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import json\n",
    "import os\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "from src import funciones_transformacion as f_transform\n",
    "from src import funciones_bbdd as f_bbdd\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El primer paso es obtener el listado de los archivos para ambas plataformas en las que se encuentren las descripciones de las ofertas de trabajo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../scraping_infojobs/datos_limpios/infojobs_data_analyst.csv',\n",
       " '../scraping_infojobs/datos_limpios/infojobs_data_engineer.csv',\n",
       " '../scraping_infojobs/datos_limpios/infojobs_data_science.csv',\n",
       " '../scraping_linkedin/data/datos_descripcion_ofertas/linkedin_data_analyst.csv',\n",
       " '../scraping_linkedin/data/datos_descripcion_ofertas/linkedin_data_engineer.csv',\n",
       " '../scraping_linkedin/data/datos_descripcion_ofertas/linkedin_data_science.csv']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lista_ofertas=[]\n",
    "\n",
    "# Files de infojobs\n",
    "folder_path_infojobs = \"../scraping_infojobs/datos_limpios/\"\n",
    "files_infojobs = os.listdir(folder_path_infojobs)\n",
    "[lista_ofertas.append(folder_path_infojobs + file) for file in files_infojobs]\n",
    "\n",
    "# Files de linkedin\n",
    "folder_path_linkedin = \"../scraping_linkedin/data/datos_descripcion_ofertas/\"\n",
    "files_linkedin = os.listdir(folder_path_linkedin)\n",
    "[lista_ofertas.append(folder_path_linkedin + file) for file in files_linkedin]\n",
    "\n",
    "lista_ofertas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El siguiente función de `extracion_datos_descripciones` se encargá de extraer de la descripción los años de experiencia que piden, el nivel de inglés, las hard skills necesarias, las hard skill deseables y un porcentaje de ajuste. Este porcetaje representa cuanto se ajusta la descripción de la oferta en cuanto al puesto para el que se realizó la busqueda, por ejemplo, Data analyst. Todos estos datos se añadirán a un json que se irá almacenando en la carpeta `data` en este mismo directorio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "Analizando el archivo -infojobs_data_analyst.csv- de la categoria: Data analyst, con shape: (108, 9)\n",
      "Analizando el archivo -infojobs_data_engineer.csv- de la categoria: Data engineer, con shape: (73, 9)\n",
      "Analizando el archivo -infojobs_data_science.csv- de la categoria: Data science, con shape: (39, 9)\n",
      "Analizando el archivo -linkedin_data_analyst.csv- de la categoria: Data analyst, con shape: (934, 8)\n",
      "Analizando el archivo -linkedin_data_engineer.csv- de la categoria: Data engineer, con shape: (948, 8)\n",
      "Analizando el archivo -linkedin_data_science.csv- de la categoria: Data science, con shape: (745, 8)\n"
     ]
    }
   ],
   "source": [
    "f_transform.extracion_datos_descripciones(lista_ofertas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Limpieza de los JSON**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A través de la API se han generado para cada oferta una lista de hard skills necesarias y deseables, sin embargo, hay que repasar esta lista para quedarnos únicamente con las hard skills y limpiar algunas soft skills que se han colado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extrayendo skills del df transformed_infojobs_data_analyst.json ...\n",
      "Extrayendo skills del df transformed_infojobs_data_engineer.json ...\n",
      "Extrayendo skills del df transformed_infojobs_data_science.json ...\n",
      "Extrayendo skills del df transformed_linkedin_data_analyst.json ...\n",
      "Extrayendo skills del df transformed_linkedin_data_engineer.json ...\n",
      "Extrayendo skills del df transformed_linkedin_data_science.json ...\n",
      "\n",
      "Completado! Hemos obtenido un total de 3210 skills\n"
     ]
    }
   ],
   "source": [
    "folder_path = \"data/response_openai/\"\n",
    "df_files = os.listdir(folder_path)\n",
    "skills = []\n",
    "\n",
    "for file in df_files:\n",
    "    print(f\"Extrayendo skills del df {file} ...\")\n",
    "    df_data_transform = pd.read_json(folder_path + file)\n",
    "    df_expanded = pd.json_normalize(df_data_transform['transformacion'])\n",
    "    df_final = pd.concat([df_data_transform.drop(columns=['transformacion']), df_expanded], axis=1)\n",
    "\n",
    "    for i in range(df_final.shape[0]):\n",
    "        skills += df_final[\"skills_necesarias\"].iloc[i]\n",
    "        skills += df_final[\"skills_valoradas\"].iloc[i]\n",
    "\n",
    "set_skills = set(skills)\n",
    "print(f\"\\nCompletado! Hemos obtenido un total de {len(set_skills)} skills\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez tenemos la lista de todas las skills recopiladas para todas las ofertas filtramos de nuevo con la API de OpenAI cuales son relamente las que nos interesan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "lista_skills = list(set_skills)\n",
    "lista_respuesta_skills = f_transform.extraer_hard_skills(lista_skills)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hemos reducido las hard skills a un total de 363\n"
     ]
    }
   ],
   "source": [
    "if lista_respuesta_skills.startswith(\"```python\\n\") and lista_respuesta_skills.endswith(\"\\n```\"):\n",
    "        respuesta = lista_respuesta_skills[10:-4]\n",
    "lista_resultado = ast.literal_eval(respuesta)\n",
    "lista_hard_skills = list(set(lista_resultado))\n",
    "\n",
    "with open('data/lista_hard_skills.pkl', 'wb') as archivo:\n",
    "    pickle.dump(lista_hard_skills, archivo)\n",
    "\n",
    "print(f\"Hemos reducido las hard skills a un total de {len(lista_hard_skills)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nuevo filtrado de las skills"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez hemos recopilado todas las skills de todas las ofertas y hemos extraido una lista de las verdaderamente relevantes el siguiente paso es ir recorriendo todas las ofertas y ver si tanto las skills necesarias como las valoradas se encuentran en dicha lista. \n",
    "\n",
    "Comprobaremos para cada oferta si sus skills están en la lista, o si se parecen a alguna skill de la lista devolviendo asi un diccionario de conversion, las que pasen en filtro se quedarán y el resto serán eliminadas. Guardaremos nuevos archivos de ofertas con las skills finales que se almacenarán en `data/skills_filtradas`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 108/108 [00:06<00:00, 17.83it/s]\n",
      "100%|██████████| 73/73 [00:02<00:00, 24.56it/s]\n",
      "100%|██████████| 39/39 [00:01<00:00, 31.91it/s]\n",
      "100%|██████████| 200/200 [00:11<00:00, 16.73it/s]\n",
      "100%|██████████| 200/200 [00:12<00:00, 15.63it/s]\n",
      "100%|██████████| 200/200 [00:10<00:00, 18.41it/s]\n"
     ]
    }
   ],
   "source": [
    "diccionario_conversion_general = f_transform.filtracion_hard_skills()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#diccionario_conversion_general"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **2. Carga de datos en MongoDB**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez hemos limpiado las skills de todas las ofertas de trabajo y las hemos almacenado en los correspondientes `json` el siguiente paso es subir todos estos datos a una base de datos en la nube. En este caso al ser tablas no relacionales hemos escogido MongoDB y los datos se subiran a MongoDB Atlas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pinged your deployment. You successfully connected to MongoDB!\n",
      "Insertando file: filtered_transformed_infojobs_data_analyst.json\n",
      "Insertando file: filtered_transformed_infojobs_data_engineer.json\n",
      "Insertando file: filtered_transformed_infojobs_data_science.json\n",
      "Insertando file: filtered_transformed_linkedin_data_analyst.json\n",
      "Insertando file: filtered_transformed_linkedin_data_engineer.json\n",
      "Insertando file: filtered_transformed_linkedin_data_science.json\n",
      "---- Insercion en la base de datos finalizada ----\n"
     ]
    }
   ],
   "source": [
    "f_bbdd.cargar_datos_mongo()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
