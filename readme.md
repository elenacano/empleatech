![alt text](imagenes/portada.png)


# Descripci칩n

Empleatech es una innovadora plataforma de b칰squeda de empleo dise침ada para
optimizar y personalizar la experiencia de los candidatos en la b칰squeda de oportunidades laborales. Actualmente, las plataformas tradicionales como LinkedIn e InfoJobs presentan filtros imprecisos, lo que genera frustraci칩n y p칠rdida de tiempo en los usuarios.

Nuestra soluci칩n recopila y analiza datos clave, como nivel de ingl칠s, a침os de experiencia y habilidades t칠cnicas (hard skills), para recomendar ofertas de trabajo alineadas con el perfil del usuario.

El proyecto ha sido desarrollado con un pipeline ETL estructurado para la extracci칩n, transformaci칩n y carga de datos de ofertas de empleo obtenidas de LinkedIn e InfoJobs. Posteriormente, un sistema de recomendaci칩n basado en machine learning optimiza la b칰squeda y presenta los resultados a trav칠s de una interfaz en Streamlit.

Si lo que desea es visitar la p치gina web e interactuar con el recomendador de ofertas de empleo lo pude encontrar en: [https://empleatech.streamlit.app/](https://empleatech.streamlit.app/), le animamos a echarle un ojo 游댌.



# Objetivos

1. **Extracci칩n y procesamiento de datos:**
    - Web scraping de LinkedIn mediante Scrapy y BeautifulSoup.
    - Web scraping de InfoJobs mediante Selenium.
    
2. **Transformaci칩n de datos:**
    -   Uso de OpenAI para extraer habilidades, nivel de ingl칠s y experiencia de las descripciones de las ofertas.
    - Agrupaci칩n de t칠rminos similares para mejorar la precisi칩n del sistema de recomendaci칩n.
    - Filtrado de skills, qued치ndonos con aquellas con un m칤nimo de apariciones.

3. **Almacenamiento y consulta de datos:**
    - Uso de MongoDB Atlas para gestionar y almacenar la informaci칩n de las ofertas de trabajo y las skills.

4. **Implementaci칩n del recomendador:**
    - Modelo basado en similitud del coseno para analizar la compatibilidad entre el perfil del usuario y las ofertas.
    - Ajuste din치mico de filtros seg칰n habilidades, experiencia y nivel de ingl칠s.

5. **Despliegue de la aplicaci칩n:**
    - Creaci칩n de una interfaz intuitiva en Streamlit para que los usuarios puedan interactuar con las recomendaciones.


# Instalaci칩n
1. Clonar el repositorio:

    ```bash
    git clone https://github.com/elenacano/empleatech.git
    cd empleatech
    ```

2. Crear y activar un entorno virtual:
    ```bash
    python -m venv env
    source env/bin/activate  # macOS/Linux
    env\Scripts\activate  # Windows
    ```
3. Instalar dependencias:
    ```bash
    pip install -r requirements.txt
    ```

# Funcionamiento paso a paso:

## **1. Extracci칩n**

### **Scrapeo de LinkedIn**

Para llevar a cabo el crapeo de LinkedIn hay que hacerlo en dos fases. 

La primera fase se realiza con scrapy, tenemos que acceder a la carpeta `scraping_linkedin\linkedin` y ejecutar el archivo run_spider.py. Autom치ticamente se almacenaran dentro de `scraping_linkedin\linkedin\data` tres archivos csv los cuales contienen la informaci칩n b치sica de las ofertas de trabajo devueltas por un endpoint de la API de LinkedIn. Sin embargo, estas ofertas no est치n completas y necesitamos la descripci칩n. Por lo tanto, dentro de estos csv podemos encontrar el link a cada oferta de donde podemos sacar la descripci칩n y m치s detalles. 

La segunda fase consite en la obtenci칩n de la informaci칩n completa para cada oferta. Este proceso se lleva a cabo ejecutando el notebook `scraping_linkedin\detailed_job_info.ipynb` por lo que el siguiente paso el hacer un *run all* de dicho notebook. Una vez ejecutado, dentro de la carpeta `scraping_linkedin\data\datos_descripcion_ofertas` encontraremos tres csv, uno por cada empleo, con datos detallados de cada oferta: empleo, plataforma, titulo_oferta, empresa, fecha_publicacion, tipo_empleo, url_oferta,descripcion_original, descripcion. En este punto habremos concluido el scrapeo de las ofertas de LinkedIn.

### **Scrapeo de Infojobs**

La ejecucui칩n de este scrapeo es un poco m치s tediosa, pues tras intentarlo de n칰merosas formas los captchas siempre terminaban por detectar el programa como un bot y no permit칤an el acceso a las ofertas. Por lo tanto, como se ha visto que el n칰mero de ofertas para cada empleo en esta plataforma no suele ser m치s de 100 se ha implementado un scrapeo supervisado con Selenium. El programa accede a trav칠s de una cuenta de infojobs y comienza la b칰squeda imitando el comportamiento humano para evitar ser detetado, sin embargo, puede saltar un captha por lo que hay que estar atento y resolverlo r치pidamente. A pesar de ser un inconveniente no siempre saltan y ahorramos mucho m치s tiempo que si hicieramos la recogida de datos a mano. En un futuro se plantea obtener la informaci칩n de las ofertas de Infojobs a trav칠s de una API de pago. 

Lo 칰nico que hay que hacer es rellenar las variables de `scraping_infojobs\.env.txt` con un usuario y contrase침a de Infojobs, (a poder ser uno nuevo y no el personal por si acaso acaba banneado por actividad sospechosa). Una vez completadas las variables de entornos ejecutamos el notebook `scraping_infojobs\selenium_infojobs.ipynb` y los datos de todas las ofertas se ir치 almacenando en `scraping_infojobs\datos_crudo`. Finalmente para llevar a cabo una primera limpieza ejecutamos el notebook `scraping_infojobs\limpieza_datos.ipynb`, se estraeran datos relevantes de las cabeceras de las ofertas y se har치 una limpieza de las descripciones, los datos limpios son almacenados en `scraping_infojobs\datos_limpios`. Y as칤 se da por concluido todo elproceso de extracci칩n de las ofertas.


## **2. Transformaci칩n**

El proceso de transformaci칩n y limpeza se lleva a cabo en la carpeta `limpieza_y_carga`, dentro se encontrar치 un notebook el cual se encarga de todo el proceso. 

El primer paso consiste en recorrer todos los archivos obtenidos en el paso anterior y de cada descripci칩n extraer los a침os de experiencia que piden, el nivel de ingl칠s, las hard skills necesarias, las hard skill deseables y un porcentaje de ajuste. Este porcetaje representa cuanto se ajusta la descripci칩n de la oferta en cuanto al puesto para el que se realiz칩 la busqueda, por ejemplo, Data analyst. Este proceso se lleva a cabo mediante la API de OpenAI con el modelo gpt-4o-mini. Para poder ejecutar el notebook necesitar darte de alta en [https://platform.openai.com/docs/overview](https://platform.openai.com/docs/overview). Una vez hayas obtenido tu API key rellena el campo *apikey_openai* en `limpieza_y_carga\.env.txt` y ya podre치s ejecutar la primera parte del notebook para la extracci칩n de las descripciones.

Despu칠s se lleva a cabo la limpieza de los JSON generados, lo agrupan mismas skills con distintos nombres y se filtran aquellas irrelevantes, una vez las skills est치 filtradas se guardan los archivos en la carpeta `limpieza_y_carga\data\skills_filtradas`. En total entre todas las ofertas hay una 3000 skills, tenemos que reducir este n칰mero, por lo que nos vamos a quedar con las skills que tengan un m칤nimo de 5 apariciones para cada tipo de empleo. Una vez tenemos esta lista, filtramos de nuevo las ofertas y toda aquella skill que no est칠 dentro de la lista de skill relevantes se elimina. Las ofertas filtradas ya con las skills relevantes se almacenan en `limpieza_y_carga\data\skills_filtradas_relevantes`. Adem치s tambi칠n se genera una lista con todas las skills relevantes que se puede consultar en `limpieza_y_carga\data\lista_hard_skills.pkl`. Como 칰ltimo paso las skills de esta lista son divididas en distintas categor칤as, para que la visualizaci칩n de estas en la aplicaci칩n sea m치s amigable, aunque este sea un paso posterior ya hay que dejarlo preparado. La lista de skills categorizadas se encuentra en `limpieza_y_carga\data\diccionario_skills_categorizadas.pkl`.


## **3. Carga a la BBDD**
Para este proyecto hemos escogido una base de datos NoSQL, almacenaremos los datos en colecciones en el servidor de MongoAtlas. Para ello es necesario registrarse en [https://www.mongodb.com/es/cloud/atlas/register](https://www.mongodb.com/es/cloud/atlas/register). Creamos un usuario con nombre *root* y una password, esta contrase침a la debemos a침adir al archivo `limpieza_y_carga\.env.txt`. Finlmente cargamos los datos a una base de datos llamada *db_empleatech_v2*, este proceso de carga se lleva a cabo al final del notebook `limpieza_y_carga\transformacion_y_carga.ipynb`.

## **4. Recomendador**



# Dependencias
Las principales bibliotecas utilizadas son:
- pandas
- numpy
- dotenv
- requests
- tqdm
- matplotlib
- seaborn
- Scrapy
- Selenium
- BeautifulSoup
- OpenAI
- pymongo
- scikit-learn
- Streamlit

Las dependencias est치n listadas en `requirements.txt` para su instalaci칩n con pip.

# Informe final y pr칩ximos pasos
Se puede consultar un informe completo del proyecto m치s detallado en el siguiente [enlace](Memoria_Empleatech.pdf). 

## Pr칩ximos pasos:
- Expansi칩n de la base de datos con ofertas de otras plataformas como Indeed o Glassdoor.
- Automatizaci칩n del pipeline ETL para actualizaciones diarias.
- Optimizaci칩n de costos con modelos de IA alternativos.
- Mejora de la p치gina web mediante el desarrollo de un backend y frontend.
- Posibilidad de aplicar directamente a las ofertas desde la plataforma.
- Desarrollo de un sistema de registro para usuarios.
- Integraci칩n con plataformas de formaci칩n para sugerencias de cursos.

# Contacto
Para dudas, sugerencias o contacto profesional:
- GitHub: https://github.com/elenacano
- LinkedIn: https://www.linkedin.com/in/elena-cano-castillejo/